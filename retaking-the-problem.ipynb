{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4c5ec13",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-07-03T17:15:47.652730Z",
     "iopub.status.busy": "2022-07-03T17:15:47.652276Z",
     "iopub.status.idle": "2022-07-03T17:15:47.663972Z",
     "shell.execute_reply": "2022-07-03T17:15:47.663020Z"
    },
    "papermill": {
     "duration": 0.023417,
     "end_time": "2022-07-03T17:15:47.666612",
     "exception": false,
     "start_time": "2022-07-03T17:15:47.643195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/titanic/train.csv\n",
      "/kaggle/input/titanic/test.csv\n",
      "/kaggle/input/titanic/gender_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "564aabca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T17:15:47.678960Z",
     "iopub.status.busy": "2022-07-03T17:15:47.677842Z",
     "iopub.status.idle": "2022-07-03T17:15:47.708883Z",
     "shell.execute_reply": "2022-07-03T17:15:47.708164Z"
    },
    "papermill": {
     "duration": 0.038714,
     "end_time": "2022-07-03T17:15:47.710973",
     "exception": false,
     "start_time": "2022-07-03T17:15:47.672259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading the training and testing data sets\n",
    "train = pd.read_csv('../input/titanic/train.csv')\n",
    "test = pd.read_csv('../input/titanic/test.csv')\n",
    "\n",
    "PassengerId = test['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f0b5f72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T17:15:47.721021Z",
     "iopub.status.busy": "2022-07-03T17:15:47.720620Z",
     "iopub.status.idle": "2022-07-03T17:15:47.810463Z",
     "shell.execute_reply": "2022-07-03T17:15:47.809439Z"
    },
    "papermill": {
     "duration": 0.097389,
     "end_time": "2022-07-03T17:15:47.812668",
     "exception": false,
     "start_time": "2022-07-03T17:15:47.715279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "full_data = [train, test]\n",
    "\n",
    "# Some features of my own that I have added in\n",
    "# Gives the length of the name\n",
    "train['Name_length'] = train['Name'].apply(len)\n",
    "test['Name_length'] = test['Name'].apply(len)\n",
    "# Feature that tells whether a passenger had a cabin on the Titanic\n",
    "train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "\n",
    "# Feature engineering steps taken from Sina\n",
    "# Create new feature FamilySize as a combination of SibSp and Parch\n",
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "# Create new feature IsAlone from FamilySize\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "# Remove all NULLS in the Embarked column\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "# Remove all NULLS in the Fare column and create a new feature CategoricalFare\n",
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n",
    "# Create a New feature CategoricalAge\n",
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "train['CategoricalAge'] = pd.cut(train['Age'], 5)\n",
    "# Define function to extract titles from passenger names\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "# Create a new feature Title, containing the titles of passenger names\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "# Group all non-common titles into one single grouping \"Rare\"\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "for dataset in full_data:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "    \n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "    # Mapping Fare\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "    # Mapping Age\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\n",
    "# Feature selection\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n",
    "test  = test.drop(drop_elements, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51e27d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T17:15:47.822479Z",
     "iopub.status.busy": "2022-07-03T17:15:47.822128Z",
     "iopub.status.idle": "2022-07-03T17:15:47.826247Z",
     "shell.execute_reply": "2022-07-03T17:15:47.825304Z"
    },
    "papermill": {
     "duration": 0.011346,
     "end_time": "2022-07-03T17:15:47.828190",
     "exception": false,
     "start_time": "2022-07-03T17:15:47.816844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training = train\n",
    "testing = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "254a7e46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T17:15:47.838420Z",
     "iopub.status.busy": "2022-07-03T17:15:47.837892Z",
     "iopub.status.idle": "2022-07-03T17:15:47.876680Z",
     "shell.execute_reply": "2022-07-03T17:15:47.874878Z"
    },
    "papermill": {
     "duration": 0.047657,
     "end_time": "2022-07-03T17:15:47.880153",
     "exception": false,
     "start_time": "2022-07-03T17:15:47.832496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Survived      Pclass         Sex         Age       Parch        Fare  \\\n",
      "count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000   \n",
      "mean     0.383838    2.308642    0.647587    1.356902    0.381594    1.505051   \n",
      "std      0.486592    0.836071    0.477990    0.839010    0.806057    1.118148   \n",
      "min      0.000000    1.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      0.000000    2.000000    0.000000    1.000000    0.000000    0.500000   \n",
      "50%      0.000000    3.000000    1.000000    1.000000    0.000000    2.000000   \n",
      "75%      1.000000    3.000000    1.000000    2.000000    0.000000    2.000000   \n",
      "max      1.000000    3.000000    1.000000    4.000000    6.000000    3.000000   \n",
      "\n",
      "         Embarked  Name_length   Has_Cabin  FamilySize     IsAlone       Title  \n",
      "count  891.000000   891.000000  891.000000  891.000000  891.000000  891.000000  \n",
      "mean     0.361392    26.965208    0.228956    1.904602    0.602694    1.728395  \n",
      "std      0.635673     9.281607    0.420397    1.613459    0.489615    1.030039  \n",
      "min      0.000000    12.000000    0.000000    1.000000    0.000000    1.000000  \n",
      "25%      0.000000    20.000000    0.000000    1.000000    0.000000    1.000000  \n",
      "50%      0.000000    25.000000    0.000000    1.000000    1.000000    1.000000  \n",
      "75%      1.000000    30.000000    0.000000    2.000000    1.000000    2.000000  \n",
      "max      2.000000    82.000000    1.000000   11.000000    1.000000    5.000000  \n",
      "\n",
      "\n",
      "the null values are :\n",
      " Survived       0\n",
      "Pclass         0\n",
      "Sex            0\n",
      "Age            0\n",
      "Parch          0\n",
      "Fare           0\n",
      "Embarked       0\n",
      "Name_length    0\n",
      "Has_Cabin      0\n",
      "FamilySize     0\n",
      "IsAlone        0\n",
      "Title          0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "the shape is :\n",
      " (891, 12)\n",
      "\n",
      "\n",
      "the names of the training are :\n",
      " ['Survived' 'Pclass' 'Sex' 'Age' 'Parch' 'Fare' 'Embarked' 'Name_length'\n",
      " 'Has_Cabin' 'FamilySize' 'IsAlone' 'Title']\n"
     ]
    }
   ],
   "source": [
    "# Printing some info about the data\n",
    "print(training.describe())\n",
    "print('\\n\\nthe null values are :\\n', training.isnull().sum())\n",
    "print('\\n\\nthe shape is :\\n', training.shape)\n",
    "print('\\n\\nthe names of the training are :\\n', training.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b03d45f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T17:15:47.891289Z",
     "iopub.status.busy": "2022-07-03T17:15:47.890476Z",
     "iopub.status.idle": "2022-07-03T17:15:47.909817Z",
     "shell.execute_reply": "2022-07-03T17:15:47.909139Z"
    },
    "papermill": {
     "duration": 0.026596,
     "end_time": "2022-07-03T17:15:47.911637",
     "exception": false,
     "start_time": "2022-07-03T17:15:47.885041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Name_length</th>\n",
       "      <th>Has_Cabin</th>\n",
       "      <th>FamilySize</th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Survived  Pclass  Sex  Age  Parch  Fare  Embarked  Name_length  \\\n",
       "0           0       3    1    1      0     0         0           23   \n",
       "1           1       1    0    2      0     3         1           51   \n",
       "2           1       3    0    1      0     1         0           22   \n",
       "3           1       1    0    2      0     3         0           44   \n",
       "4           0       3    1    2      0     1         0           24   \n",
       "..        ...     ...  ...  ...    ...   ...       ...          ...   \n",
       "886         0       2    1    1      0     1         0           21   \n",
       "887         1       1    0    1      0     2         0           28   \n",
       "888         0       3    0    1      2     2         0           40   \n",
       "889         1       1    1    1      0     2         1           21   \n",
       "890         0       3    1    1      0     0         2           19   \n",
       "\n",
       "     Has_Cabin  FamilySize  IsAlone  Title  \n",
       "0            0           2        0      1  \n",
       "1            1           2        0      3  \n",
       "2            0           1        1      2  \n",
       "3            1           2        0      3  \n",
       "4            0           1        1      1  \n",
       "..         ...         ...      ...    ...  \n",
       "886          0           1        1      5  \n",
       "887          1           1        1      2  \n",
       "888          0           4        0      2  \n",
       "889          1           1        1      1  \n",
       "890          0           1        1      1  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90844662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T17:15:47.922822Z",
     "iopub.status.busy": "2022-07-03T17:15:47.922422Z",
     "iopub.status.idle": "2022-07-03T17:15:47.933422Z",
     "shell.execute_reply": "2022-07-03T17:15:47.932435Z"
    },
    "papermill": {
     "duration": 0.018659,
     "end_time": "2022-07-03T17:15:47.935303",
     "exception": false,
     "start_time": "2022-07-03T17:15:47.916644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we can do the predicting Process.\n",
    "X = training.drop('Survived', axis=1)\n",
    "y = training['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5c0660",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T17:15:47.946297Z",
     "iopub.status.busy": "2022-07-03T17:15:47.945370Z",
     "iopub.status.idle": "2022-07-03T17:15:49.145205Z",
     "shell.execute_reply": "2022-07-03T17:15:49.144062Z"
    },
    "papermill": {
     "duration": 1.208215,
     "end_time": "2022-07-03T17:15:49.148055",
     "exception": false,
     "start_time": "2022-07-03T17:15:47.939840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting the data into training and validation data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61753f76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T17:15:49.159319Z",
     "iopub.status.busy": "2022-07-03T17:15:49.158899Z",
     "iopub.status.idle": "2022-07-03T17:15:49.163299Z",
     "shell.execute_reply": "2022-07-03T17:15:49.162399Z"
    },
    "papermill": {
     "duration": 0.012012,
     "end_time": "2022-07-03T17:15:49.165133",
     "exception": false,
     "start_time": "2022-07-03T17:15:49.153121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# classifiers = [\n",
    "#     KNeighborsClassifier(8),\n",
    "#     SVC(probability=True),\n",
    "#     DecisionTreeClassifier(),\n",
    "#     RandomForestClassifier(),\n",
    "# \tAdaBoostClassifier(),\n",
    "#     GradientBoostingClassifier(),\n",
    "#     GaussianNB(),\n",
    "#     LinearDiscriminantAnalysis(),\n",
    "#     QuadraticDiscriminantAnalysis(),\n",
    "#     LogisticRegression()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e2bec23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T17:15:49.176891Z",
     "iopub.status.busy": "2022-07-03T17:15:49.175052Z",
     "iopub.status.idle": "2022-07-03T17:15:49.180893Z",
     "shell.execute_reply": "2022-07-03T17:15:49.179999Z"
    },
    "papermill": {
     "duration": 0.013517,
     "end_time": "2022-07-03T17:15:49.183102",
     "exception": false,
     "start_time": "2022-07-03T17:15:49.169585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for classifier in classifiers:\n",
    "#     classifier.fit(X_train, y_train)\n",
    "#     scores.append(classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86927ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T17:15:49.194290Z",
     "iopub.status.busy": "2022-07-03T17:15:49.193755Z",
     "iopub.status.idle": "2022-07-03T17:15:49.197582Z",
     "shell.execute_reply": "2022-07-03T17:15:49.196659Z"
    },
    "papermill": {
     "duration": 0.011695,
     "end_time": "2022-07-03T17:15:49.199526",
     "exception": false,
     "start_time": "2022-07-03T17:15:49.187831",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for (cls, scr) in zip(classifiers, scores):\n",
    "#     print(cls, '  ', scr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25445d9",
   "metadata": {
    "papermill": {
     "duration": 0.004195,
     "end_time": "2022-07-03T17:15:49.208407",
     "exception": false,
     "start_time": "2022-07-03T17:15:49.204212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# After trying all the classifiers above i found out that GradientBoostingClassifier is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31a6bda4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T17:15:49.219500Z",
     "iopub.status.busy": "2022-07-03T17:15:49.218749Z",
     "iopub.status.idle": "2022-07-03T17:15:50.509089Z",
     "shell.execute_reply": "2022-07-03T17:15:50.507992Z"
    },
    "papermill": {
     "duration": 1.298941,
     "end_time": "2022-07-03T17:15:50.511974",
     "exception": false,
     "start_time": "2022-07-03T17:15:49.213033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(max_iter=1000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(max_iter=1000)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7c23d3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T17:15:50.533480Z",
     "iopub.status.busy": "2022-07-03T17:15:50.532278Z",
     "iopub.status.idle": "2022-07-03T17:15:50.540277Z",
     "shell.execute_reply": "2022-07-03T17:15:50.539056Z"
    },
    "papermill": {
     "duration": 0.023465,
     "end_time": "2022-07-03T17:15:50.544967",
     "exception": false,
     "start_time": "2022-07-03T17:15:50.521502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bd1ea72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T17:15:50.565437Z",
     "iopub.status.busy": "2022-07-03T17:15:50.564646Z",
     "iopub.status.idle": "2022-07-03T17:15:50.577998Z",
     "shell.execute_reply": "2022-07-03T17:15:50.576469Z"
    },
    "papermill": {
     "duration": 0.027441,
     "end_time": "2022-07-03T17:15:50.581605",
     "exception": false,
     "start_time": "2022-07-03T17:15:50.554164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d61a1cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-03T17:15:50.602353Z",
     "iopub.status.busy": "2022-07-03T17:15:50.601596Z",
     "iopub.status.idle": "2022-07-03T17:15:50.615892Z",
     "shell.execute_reply": "2022-07-03T17:15:50.614765Z"
    },
    "papermill": {
     "duration": 0.027871,
     "end_time": "2022-07-03T17:15:50.618923",
     "exception": false,
     "start_time": "2022-07-03T17:15:50.591052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate Submission File \n",
    "GBCsubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n",
    "                            'Survived': predictions })\n",
    "GBCsubmission.to_csv(\"QuadraticDiscriminantAnalysis.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4fd033",
   "metadata": {
    "papermill": {
     "duration": 0.008613,
     "end_time": "2022-07-03T17:15:50.636882",
     "exception": false,
     "start_time": "2022-07-03T17:15:50.628269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.13293,
   "end_time": "2022-07-03T17:15:51.369049",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-07-03T17:15:38.236119",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
